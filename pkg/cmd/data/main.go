package main

import (
	"bytes"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"math/rand"
	"strings"
	"time"

	"github.com/elastic/go-elasticsearch/v5"
	"github.com/elastic/go-elasticsearch/v5/esapi"
	"github.com/go-resty/resty/v2"
)

type Record struct {
	Address struct {
		AreaCode  string `json:"area_code"`
		AreaName  string `json:"area_name"`
		CityCode  string `json:"city_code"`
		Lat       string `json:"lat"`
		Lng       string `json:"lng"`
		Name      string `json:"name"`
		ShortName string `json:"short_name"`
		ZipCode   string `json:"zip_code"`
	} `json:"address"`
	Age         string `json:"age"`
	Airlineinfo struct {
		Code string `json:"code"`
		Name string `json:"name"`
	} `json:"airlineinfo"`
	Airport struct {
		City     string `json:"city"`
		IataCode string `json:"iata_code"`
		IcaoCode string `json:"icao_code"`
		Name     string `json:"name"`
		Pinyin   string `json:"pinyin"`
	} `json:"airport"`
	Capturetime  string `json:"capturetime"`
	Citycode     string `json:"citycode"`
	Color        string `json:"color"`
	Date         string `json:"date"`
	Deviceid     string `json:"deviceid"`
	Email        string `json:"email"`
	Flightseat   string `json:"flightseat"`
	Idcard       string `json:"idcard"`
	Imei         string `json:"imei"`
	Imid         string `json:"imid"`
	Imsi         string `json:"imsi"`
	Ipv4         string `json:"ipv4"`
	Ipv6         string `json:"ipv6"`
	Job          string `json:"job"`
	Mac          string `json:"mac"`
	Meid         string `json:"meid"`
	Mobilephone  string `json:"mobilephone"`
	Name         string `json:"name"`
	Nickname     string `json:"nickname"`
	Password     string `json:"password"`
	Sex          string `json:"sex"`
	Specialphone string `json:"specialphone"`
	Telphone     string `json:"telphone"`
	Trainseat    string `json:"trainseat"`
	Traintrips   string `json:"traintrips"`
	URL          string `json:"url"`
	Useragent    string `json:"useragent"`
	Username     string `json:"username"`
	Voyage       string `json:"voyage"`
	Website      string `json:"website"`
}

type AutoGenerated struct {
	Data struct {
		Count   int      `json:"count"`
		Records []Record `json:"records"`
	} `json:"data"`
	Status struct {
		Code   string `json:"code"`
		Status string `json:"status"`
	} `json:"status"`
}

const (
	ES_ADDRESSES = "http://127.0.0.1:9200"
	ES_INDEX     = "search_test"
	ES_TYPE      = "all"
)

var (
	_     = fmt.Print
	count int
	batch int
)

func init() {
	flag.IntVar(&count, "count", 1000, "Number of documents to generate")
	flag.IntVar(&batch, "batch", 255, "Number of documents to send in one batch")
	flag.Parse()
	rand.Seed(time.Now().UnixNano())
}

func main() {
	client := resty.New()
	resp, err := client.R().
		Get("http://127.0.0.1:8001/api/v1/fakerfactory?number=1000&columns=color,job,name,sex,address,idcard,age,mobilephone,email,imid,nickname,username,password,website,url,airport,voyage,airlineinfo,traintrips,trainseat,flightseat,ipv4,ipv6,useragent,mac,imsi,imei,meid,deviceid,telphone,citycode,specialphone,capturetime,date")
	if err != nil {
		fmt.Println(err.Error())
	}
	// fmt.Println(string(resp.Body()))
	var arr AutoGenerated
	json.Unmarshal(resp.Body(), &arr)
	// fmt.Printf("%+v", arr)

	type bulkResponse struct {
		Errors bool `json:"errors"`
		Items  []struct {
			Index struct {
				ID     string `json:"_id"`
				Result string `json:"result"`
				Status int    `json:"status"`
				Error  struct {
					Type   string `json:"type"`
					Reason string `json:"reason"`
					Cause  struct {
						Type   string `json:"type"`
						Reason string `json:"reason"`
					} `json:"caused_by"`
				} `json:"error"`
			} `json:"index"`
		} `json:"items"`
	}

	var (
		buf bytes.Buffer
		res *esapi.Response
		// err error
		raw map[string]interface{}
		blk *bulkResponse

		numItems   int
		numErrors  int
		numIndexed int
		numBatches int
		currBatch  int
	)

	// Create the Elasticsearch client
	//
	addresses := make([]string, 0)
	addresses = append(addresses, ES_ADDRESSES)
	cfg := elasticsearch.Config{
		Addresses: addresses,
	}
	es, err := elasticsearch.NewClient(cfg)
	if err != nil {
		log.Fatalf("Error creating the client: %s", err)
	}

	if count%batch == 0 {
		numBatches = (count / batch)
	} else {
		numBatches = (count / batch) + 1
	}

	start := time.Now().UTC()

	// Loop over the collection
	//
	for i, a := range arr.Data.Records {
		numItems++

		currBatch = i / batch
		if i == count-1 {
			currBatch++
		}

		// Prepare the metadata payload
		//
		meta := []byte(fmt.Sprintf(`{ "index" : { "_id" : "%s", "_type" : "%s" } }%s`, a.Idcard, ES_TYPE, "\n"))
		// fmt.Printf("%s", meta) // <-- Uncomment to see the payload

		// Prepare the data payload: encode article to JSON
		//
		data, err := json.Marshal(a)
		if err != nil {
			log.Fatalf("Cannot encode article %s: %s", a.Idcard, err)
		}

		// Append newline to the data payload
		//
		data = append(data, "\n"...) // <-- Comment out to trigger failure for batch
		// fmt.Printf("%s", data)       // <-- Uncomment to see the payload

		// // Uncomment next block to trigger indexing errors -->
		// if a.ID == 11 || a.ID == 101 {
		// 	data = []byte(`{"published" : "INCORRECT"}` + "\n")
		// }
		// // <--------------------------------------------------

		// Append payloads to the buffer (ignoring write errors)
		//
		buf.Grow(len(meta) + len(data))
		buf.Write(meta)
		buf.Write(data)
		// fmt.Println(string(buf.Bytes()))

		// When a threshold is reached, execute the Bulk() request with body from buffer
		//
		// if i > 0 && i%batch == 0 || i == count-1 {
		log.Printf("> Batch %-2d of %d", currBatch, numBatches)

		res, err = es.Bulk(bytes.NewReader(buf.Bytes()), es.Bulk.WithIndex(ES_INDEX))
		if err != nil {
			log.Fatalf("Failure indexing batch %d: %s", currBatch, err)
		}
		// If the whole request failed, print error and mark all documents as failed
		//
		if res.IsError() {
			numErrors += numItems
			if err := json.NewDecoder(res.Body).Decode(&raw); err != nil {
				log.Fatalf("Failure to to parse response body: %s", err)
			} else {
				log.Printf("  Error: [%d] %s: %s",
					res.StatusCode,
					raw["error"].(map[string]interface{})["type"],
					raw["error"].(map[string]interface{})["reason"],
				)
			}
			// A successful response might still contain errors for particular documents...
			//
		} else {
			if err := json.NewDecoder(res.Body).Decode(&blk); err != nil {
				log.Fatalf("Failure to to parse response body: %s", err)
			} else {
				for _, d := range blk.Items {
					// ... so for any HTTP status above 201 ...
					//
					if d.Index.Status > 201 {
						// ... increment the error counter ...
						//
						numErrors++

						// ... and print the response status and error information ...
						log.Printf("  Error: [%d]: %s: %s: %s: %s",
							d.Index.Status,
							d.Index.Error.Type,
							d.Index.Error.Reason,
							d.Index.Error.Cause.Type,
							d.Index.Error.Cause.Reason,
						)
					} else {
						// ... otherwise increase the success counter.
						//
						numIndexed++
					}
				}
			}
		}
		// Reset the buffer and items counter
		//
		buf.Reset()
		numItems = 0
		// }
	}

	// Report the results: number of indexed docs, number of errors, duration, indexing rate
	//
	log.Println(strings.Repeat("=", 80))

	dur := time.Since(start)

	if numErrors > 0 {
		log.Fatalf(
			"Indexed [%d] documents with [%d] errors in %s (%.0f docs/sec)",
			numIndexed,
			numErrors,
			dur.Truncate(time.Millisecond),
			1000.0/float64(dur/time.Millisecond)*float64(numIndexed),
		)
	} else {
		log.Printf(
			"Sucessfuly indexed [%d] documents in %s (%.0f docs/sec)",
			numIndexed,
			dur.Truncate(time.Millisecond),
			1000.0/float64(dur/time.Millisecond)*float64(numIndexed),
		)
	}
}
